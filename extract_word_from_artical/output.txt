【AI Is Dangerous】 -----> # ***AI Is Dangerous***, but Not for the Reasons You Think

[***AI Is Dangerous***, but Not for the Reasons You Think](https://www.

【AI Is Dangerous】 -----> # ***AI Is Dangerous***, but Not for the Reasons You Think

[***AI Is Dangerous***, but Not for the Reasons You Think](https://www.

【dope Pope】 -----> (Laughter) It's in the headlines pretty much every day, sometimes because of really cool things like discovering new molecules for medicine or that ***dope Pope*** in the white puffer coat.

【puffer coat】 -----> (Laughter) It's in the headlines pretty much every day, sometimes because of really cool things like discovering new molecules for medicine or that dope Pope in the white ***puffer coat***.

【divorce】 -----> (00:33) But other times the headlines have been really dark, like that chatbot telling that guy that he should ***divorce*** his wife or that AI meal planner app proposing a crowd pleasing recipe featuring chlorine gas.

【doomsday scenarios】 -----> And in the background, we've heard a lot of talk about ***doomsday scenarios***, existential risk and the singularity, with letters being written and events being organized to make sure that doesn't happen.

【nasty】 -----> But what I do know is that there's some pretty ***nasty*** things going on right now, because AI doesn't exist in a vacuum.

【vacuum】 -----> But what I do know is that there's some pretty nasty things going on right now, because AI doesn't exist in a ***vacuum***.

【consent】 -----> Their training data uses art and books created by artists and authors without their ***consent***.

【discriminate】 -----> And its deployment can ***discriminate*** against entire communities.

【metal】 -----> (01:50) But let's start with sustainability, because that cloud that AI models live on is actually made out of ***metal***, plastic, and powered by vast amounts of energy.

【iceberg】 -----> And so this is probably only the tip of the ***iceberg***, even if it is a melting one.

【piling up】 -----> And as we're putting in these models into cell phones and search engines and smart fridges and speakers, the environmental costs are really ***piling up*** quickly.

【tangible】 -----> So instead of focusing on some future existential risks, let's talk about current ***tangible*** impacts and tools we can create to measure and mitigate these impacts.

【parallel】 -----> (03:50) I helped create CodeCarbon, a tool that runs in ***parallel*** to AI training code that estimates the amount of energy it consumes and the amount of carbon it emits.

【informed choices】 -----> And using a tool like this can help us make ***informed choices***, like choosing one model over the other because it's more sustainable, or deploying AI models on renewable energy, which can drastically reduce their emissions.

【drastically】 -----> And using a tool like this can help us make informed choices, like choosing one model over the other because it's more sustainable, or deploying AI models on renewable energy, which can ***drastically*** reduce their emissions.

【proof】 -----> And if you want to sue someone, you tend to need ***proof***, right?

【Spawning】 -----> So ***Spawning***.

【massive】 -----> And it lets you search these ***massive*** data sets to see what they have on you.

【infringement】 -----> And while it can be interesting for people like you and me to search these data sets, for artists like Karla Ortiz, this provides crucial evidence that her life's work, her artwork, was used for training AI models without her consent, and she and two artists used this as evidence to file a class action lawsuit against AI companies for copyright ***infringement***.

【partnered up】 -----> ai ***partnered up*** with Hugging Face, the company where I work at, to create opt-in and opt-out mechanisms for creating these data sets.

【mechanisms】 -----> ai partnered up with Hugging Face, the company where I work at, to create opt-in and opt-out ***mechanisms*** for creating these data sets.

【stereotypes】 -----> Formally speaking, it's when AI models encode patterns and beliefs that can represent ***stereotypes*** or racism and sexism.

【racism】 -----> Formally speaking, it's when AI models encode patterns and beliefs that can represent stereotypes or ***racism*** and sexism.

【sexism】 -----> Formally speaking, it's when AI models encode patterns and beliefs that can represent stereotypes or racism and ***sexism***.

【experienced this firsthand】 -----> Joy Buolamwini, ***experienced this firsthand*** when she realized that AI systems wouldn't even detect her face unless she was wearing a white-colored mask.

【Digging deeper】 -----> (06:23) ***Digging deeper***, she found that common facial recognition systems were vastly worse for women of color compared to white men.

【accusations】 -----> And when biased models like this are deployed in law enforcement settings, this can result in false ***accusations***, even wrongful imprisonment, which we've seen happen to multiple people in recent months.

【imprisonment】 -----> And when biased models like this are deployed in law enforcement settings, this can result in false accusations, even wrongful ***imprisonment***, which we've seen happen to multiple people in recent months.

【forensic sketch】 -----> And for example, for image generation systems, if they're used in contexts like generating a ***forensic sketch*** based on a description of a perpetrator, they take all those biases and they spit them back out for terms like dangerous criminal, terrorists or gang member,
(07:18) which of course is super dangerous when these tools are deployed in society.

【perpetrator】 -----> And for example, for image generation systems, if they're used in contexts like generating a forensic sketch based on a description of a ***perpetrator***, they take all those biases and they spit them back out for terms like dangerous criminal, terrorists or gang member,
(07:18) which of course is super dangerous when these tools are deployed in society.

【through the lens of professions】 -----> And so in order to understand these tools better, I created this tool called the Stable Bias Explorer, which lets you explore the bias of image generation models ***through the lens of professions***.

【masculinity】 -----> And the thing is, is that we looked at all these different image generation models and found a lot of the same thing: significant representation of whiteness and ***masculinity*** across all 150 professions that we looked at, even if compared to the real world, the US Labor Bureau of Statistics.

【legislation】 -----> And sadly, my tool hasn't been used to write ***legislation*** yet.

【are of interest】 -----> But I recently presented it at a UN event about gender bias as an example of how we can make tools for people from all walks of life, even those who don't know how to code, to engage with and better understand AI because we use professions, but you can use any terms that ***are of interest*** to you.

【woven into】 -----> (08:37) And as these models are being deployed, are being ***woven into*** the very fabric of our societies, our cell phones, our social media feeds, even our justice systems and our economies have AI in them.

【guardrails】 -----> Start creating ***guardrails*** to protect society and the planet.

【Legislators】 -----> (09:26) ***Legislators*** who really need information to write laws, can use these tools to develop new regulation mechanisms or governance for AI as it gets deployed into society.

【mechanisms】 -----> (09:26) Legislators who really need information to write laws, can use these tools to develop new regulation ***mechanisms*** or governance for AI as it gets deployed into society.

【governance】 -----> (09:26) Legislators who really need information to write laws, can use these tools to develop new regulation mechanisms or ***governance*** for AI as it gets deployed into society.

【misrepresent】 -----> And users like you and me can use this information to choose AI models that we can trust, not to ***misrepresent*** us and not to misuse our data.

【misuse】 -----> And users like you and me can use this information to choose AI models that we can trust, not to misrepresent us and not to ***misuse*** our data.

【existential risks】 -----> I said that focusing on AI's future ***existential risks*** is a distraction from its current, very tangible impacts and the work we should be doing right now, or even yesterday, for reducing these impacts.

【distraction】 -----> I said that focusing on AI's future existential risks is a ***distraction*** from its current, very tangible impacts and the work we should be doing right now, or even yesterday, for reducing these impacts.

【tangible】 -----> I said that focusing on AI's future existential risks is a distraction from its current, very ***tangible*** impacts and the work we should be doing right now, or even yesterday, for reducing these impacts.

【collectively】 -----> (10:09) We're building the road as we walk it, and we can ***collectively*** decide what direction we want to go in together.

